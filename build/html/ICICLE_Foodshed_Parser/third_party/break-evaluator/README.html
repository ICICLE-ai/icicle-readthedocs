<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Break Evaluator &#8212; ICICLE-READTHEDOCS  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=cb25574f" />
    <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="break-evaluator">
<h1>Break Evaluator<a class="headerlink" href="#break-evaluator" title="Link to this heading">¶</a></h1>
<p>Evaluator for the <a class="reference external" href="https://github.com/allenai/Break">Break</a> dataset (AI2 Israel).<br />
Used in both the Break and Break High-level leaderboards.</p>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Link to this heading">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%<span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="w"> </span>python3.7<span class="w"> </span>scripts/evaluate_predictions.py<span class="w"> </span>
--dataset_file<span class="o">=</span>/labels/labels.csv<span class="w"> </span><span class="se">\</span>
--preds_file<span class="o">=</span>/predictions/predictions.csv<span class="w"> </span><span class="se">\</span>
--no_cache<span class="w"> </span><span class="se">\</span>
--output_file_base<span class="o">=</span>/results/results<span class="w"> </span><span class="se">\</span>
--metrics<span class="w"> </span>ged_scores<span class="w"> </span>exact_match<span class="w"> </span>sari<span class="w"> </span>normalized_exact_match<span class="w"> </span><span class="se">\</span>
<span class="w">				</span>
%<span class="w"> </span>cat<span class="w"> </span>results/results_metrics.json
<span class="o">{</span><span class="s2">&quot;exact_match&quot;</span>:<span class="w"> </span><span class="m">0</span>.24242424242424243,<span class="w"> </span><span class="s2">&quot;sari&quot;</span>:<span class="w"> </span><span class="m">0</span>.7061778423719823,<span class="w"> </span><span class="s2">&quot;ged&quot;</span>:<span class="w"> </span><span class="m">0</span>.4089606835211786,<span class="w"> </span><span class="s2">&quot;normalized_exact_match&quot;</span>:<span class="w"> </span><span class="m">0</span>.32323232323232326<span class="o">}</span>
</pre></div>
</div>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Link to this heading">¶</a></h2>
<section id="input">
<h3>Input<a class="headerlink" href="#input" title="Link to this heading">¶</a></h3>
<p>The evaluation script recieves as input a Break <code class="docutils literal notranslate"><span class="pre">dataset_file</span></code> which is a CSV file containing the correct <em>labels</em>. Additionally, it should receive <code class="docutils literal notranslate"><span class="pre">preds_file</span></code>, a CSV file containing a model’s <em>predictions</em>, ordered according to <code class="docutils literal notranslate"><span class="pre">dataset_file</span></code>. The <code class="docutils literal notranslate"><span class="pre">output_file_base</span></code> indicates the file to which the evaluation output be saved. Last <code class="docutils literal notranslate"><span class="pre">metrics</span></code> indicates which evaluation metrics should be included out of <code class="docutils literal notranslate"><span class="pre">ged_scores,</span> <span class="pre">exact_match,</span> <span class="pre">sari,</span> <span class="pre">normalized_exact_match</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tmp</span></code> directory contains examples of <a class="reference external" href="https://github.com/allenai/break-evaluator/blob/master/tmp/labels/labels.csv"><code class="docutils literal notranslate"><span class="pre">dataset_file</span></code></a> and <a class="reference external" href="https://github.com/allenai/break-evaluator/blob/master/tmp/predictions/predictions.csv"><code class="docutils literal notranslate"><span class="pre">preds_file</span></code></a>.</p>
</section>
<section id="output">
<h3>Output<a class="headerlink" href="#output" title="Link to this heading">¶</a></h3>
<p>The evaluation output will be saved to <code class="docutils literal notranslate"><span class="pre">output_file_base_metrics.json</span></code></p>
</section>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">¶</a></h2>
<p>To run the evaluation script locally, using a <em>conda virtual environment</em>, do the following:</p>
<ol class="arabic simple">
<li><p>Create a virtual environment</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="p">[</span><span class="n">ENV_NAME</span><span class="p">]</span> <span class="n">python</span><span class="o">=</span><span class="mf">3.7</span>
<span class="n">conda</span> <span class="n">activate</span> <span class="p">[</span><span class="n">ENV_NAME</span><span class="p">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Install requirements</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span> 
<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">en_core_web_sm</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Run in shell</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PYTHONPATH</span><span class="o">=</span><span class="s2">&quot;.&quot;</span> <span class="n">python3</span><span class="mf">.7</span> <span class="n">scripts</span><span class="o">/</span><span class="n">evaluate_predictions</span><span class="o">.</span><span class="n">py</span> 
<span class="o">--</span><span class="n">dataset_file</span><span class="o">=/</span><span class="n">labels</span><span class="o">/</span><span class="n">labels</span><span class="o">.</span><span class="n">csv</span> \
<span class="o">--</span><span class="n">preds_file</span><span class="o">=/</span><span class="n">predictions</span><span class="o">/</span><span class="n">predictions</span><span class="o">.</span><span class="n">csv</span> \
<span class="o">--</span><span class="n">no_cache</span> \
<span class="o">--</span><span class="n">output_file_base</span><span class="o">=/</span><span class="n">results</span><span class="o">/</span><span class="n">results</span> \
<span class="o">--</span><span class="n">metrics</span> <span class="n">ged_scores</span> <span class="n">exact_match</span> <span class="n">sari</span> <span class="n">normalized_exact_match</span> \
</pre></div>
</div>
</section>
<section id="docker">
<h2>Docker<a class="headerlink" href="#docker" title="Link to this heading">¶</a></h2>
<p>We build an evaluator image using Docker, and the specified Dockerfile.</p>
<section id="build">
<h3>Build<a class="headerlink" href="#build" title="Link to this heading">¶</a></h3>
<p>To build the break-evaluator image:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">build</span> <span class="o">--</span><span class="n">tag</span> <span class="k">break</span><span class="o">-</span><span class="n">evaluator</span> <span class="o">.</span>
</pre></div>
</div>
</section>
<section id="run">
<h3>Run<a class="headerlink" href="#run" title="Link to this heading">¶</a></h3>
<p>Our evaluator should receive three files as input, the dataset true labels, the model’s prediction file and the path to the output file. We therefore <em>bind mount</em> the relevant files when using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code>.
The specific volume mounts, given our relevant files are storem in <code class="docutils literal notranslate"><span class="pre">tmp</span></code>, will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="n">v</span> <span class="s2">&quot;$(pwd)&quot;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">results</span><span class="o">/</span><span class="p">:</span><span class="o">/</span><span class="n">results</span><span class="p">:</span><span class="n">rw</span>
<span class="o">-</span><span class="n">v</span> <span class="s2">&quot;$(pwd)&quot;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">predictions</span><span class="o">/</span><span class="p">:</span><span class="o">/</span><span class="n">predictions</span><span class="p">:</span><span class="n">ro</span>
<span class="o">-</span><span class="n">v</span> <span class="s2">&quot;$(pwd)&quot;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">labels</span><span class="o">/</span><span class="p">:</span><span class="o">/</span><span class="n">labels</span><span class="p">:</span><span class="n">ro</span>
</pre></div>
</div>
<p>The full run command being:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sudo</span> <span class="n">docker</span> <span class="n">run</span> <span class="o">-</span><span class="n">it</span> <span class="o">-</span><span class="n">v</span> <span class="s2">&quot;$(pwd)&quot;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">results</span><span class="o">/</span><span class="p">:</span><span class="o">/</span><span class="n">results</span><span class="p">:</span><span class="n">rw</span> <span class="o">-</span><span class="n">v</span> <span class="s2">&quot;$(pwd)&quot;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">predictions</span><span class="o">/</span><span class="p">:</span><span class="o">/</span><span class="n">predictions</span><span class="p">:</span><span class="n">ro</span> <span class="o">-</span><span class="n">v</span> <span class="s2">&quot;$(pwd)&quot;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">labels</span><span class="o">/</span><span class="p">:</span><span class="o">/</span><span class="n">labels</span><span class="p">:</span><span class="n">ro</span> <span class="k">break</span><span class="o">-</span><span class="n">evaluator</span> <span class="n">bash</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;python3.7 scripts/evaluate_predictions.py --dataset_file=/labels/labels.csv --preds_file=/predictions/predictions.csv --no_cache --output_file_base=/results/results --metrics ged_scores exact_match sari normalized_exact_match&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="beaker">
<h2>Beaker<a class="headerlink" href="#beaker" title="Link to this heading">¶</a></h2>
<p>To add a Beaker image of the evaluator run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">beaker</span> <span class="n">image</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="k">break</span><span class="o">-</span><span class="n">evaluator</span><span class="o">-</span><span class="n">YYYY</span><span class="o">-</span><span class="n">MM</span><span class="o">-</span><span class="n">DD</span> <span class="k">break</span><span class="o">-</span><span class="n">evaluator</span><span class="p">:</span><span class="n">latest</span>
</pre></div>
</div>
</section>
<section id="evaluation-metircs">
<h2>Evaluation Metircs<a class="headerlink" href="#evaluation-metircs" title="Link to this heading">¶</a></h2>
<p>To learn more about the evaluation metrics used for <a class="reference external" href="https://allenai.github.io/Break/">Break</a>, please refer to the paper <a class="reference external" href="https://arxiv.org/abs/2001.11770">“Break It Down: A Question Understanding Benchmark” (Wolfson et al., TACL 2020)</a>.<br />
The <em>“Normalized Exact Match”</em> metric, is a newly introduced evaluation metric for QDMR that will be included in future work. It compares two QDMRs by normalizing their respective graphs: further decomposing steps; ordering chains of “filter” operations; lemmatizing step noun phrases; etc.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">ICICLE-READTHEDOCS</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../test.html">HELLO WORLD</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, ICICLE.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.2.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../../_sources/ICICLE_Foodshed_Parser/third_party/break-evaluator/README.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>